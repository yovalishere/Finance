{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apache Spark is used in this notebook to improve the speed of data processing, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting findspark\n",
      "  Downloading findspark-2.0.1-py2.py3-none-any.whl (4.4 kB)\n",
      "Installing collected packages: findspark\n",
      "Successfully installed findspark-2.0.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install findspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "|hello|\n",
      "+-----+\n",
      "|spark|\n",
      "+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test Spark installation\n",
    "import findspark\n",
    "findspark.init()\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "spark=SparkSession.builder.getOrCreate()\n",
    "df=spark.sql(\"select 'spark' as hello\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Open: string (nullable = true)\n",
      " |-- High: string (nullable = true)\n",
      " |-- Low: string (nullable = true)\n",
      " |-- Close: string (nullable = true)\n",
      " |-- Adj Close: string (nullable = true)\n",
      " |-- Volume: string (nullable = true)\n",
      " |-- symbol: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Read all_data.csv\n",
    "spark=SparkSession.builder.master('local')\\\n",
    "                  .appName(\"SparkTimeSeries\")\\\n",
    "                  .getOrCreate()\n",
    "\n",
    "df=spark.read.option(\"header\",True)\\\n",
    "                .csv(\"all_data.csv\")\n",
    "\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------+\n",
      "|count(DISTINCT symbol)|\n",
      "+----------------------+\n",
      "|                   100|\n",
      "+----------------------+\n",
      "\n",
      "+----+----+---+-----+---------+------+------+\n",
      "|Open|High|Low|Close|Adj Close|Volume|symbol|\n",
      "+----+----+---+-----+---------+------+------+\n",
      "|   0|   0|  0|    0|        0|     0|     0|\n",
      "+----+----+---+-----+---------+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "############################################################ Data cleaning ##########################################\n",
    "from pyspark.sql.functions import desc\n",
    "from pyspark.sql.functions import col,isnan,when,count\n",
    "from pyspark.sql.functions import countDistinct\n",
    "\n",
    "# Verify # of ticker symbols\n",
    "Num_symbols=df.select(countDistinct(\"symbol\"))\n",
    "Num_symbols.show()\n",
    "\n",
    "# Count no. of Null in each col\n",
    "Count_null=df.select([count(when(isnan(c)| col(c).isNull()| (df[c] == 0),c)).alias(c) for c in df.columns])\n",
    "Count_null.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+------------------+------------------+------------------+------------------+----------+------+\n",
      "|              Open|              High|               Low|             Close|         Adj Close|    Volume|symbol|\n",
      "+------------------+------------------+------------------+------------------+------------------+----------+------+\n",
      "|142.99000549316406| 146.4499969482422| 142.1199951171875|145.49000549316406|145.28819274902344|71185600.0|  AAPL|\n",
      "| 144.0800018310547| 148.9499969482422|            143.25|148.47000122070312|148.26405334472656|78140700.0|  AAPL|\n",
      "|149.77999877929688|150.86000061035156| 148.1999969482422| 150.1699981689453|149.96170043945312|76259900.0|  AAPL|\n",
      "|150.74000549316406|151.57000732421875| 146.6999969482422|147.07000732421875|146.86599731445312|81420900.0|  AAPL|\n",
      "| 147.9199981689453|151.22999572753906|146.91000366210938|             151.0|150.79054260253906|82982400.0|  AAPL|\n",
      "+------------------+------------------+------------------+------------------+------------------+----------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Only extract AAPL\n",
    "df_aapl=df.filter(df['symbol']=='AAPL')\n",
    "df_aapl.show(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "82d4c6f819cf47785f735f902f00da8643513d08dab4f4c7470bccf934b8d2d6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
